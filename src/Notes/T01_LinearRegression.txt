Training Set & Test Set
   Lets imagine that u have the task to predict the selling price of cars
   and that is your independent variable and the dependent variable is mileage and age of the car
   and the data have 20 cars in total.
   usually we keep 20% of the data to test and rest for the training.
   so, we will use the training set to build the model
   (lets in this case we are building linear regression -> Y = b0 + b1X1 + b2X2)
   and then we will take the data from the test set and start comparing between predicted and actual price
   based on we will decide how good model is.

Feature Scaling
   This is always applied to column.(not applied to cross column)
   It is a technique to standardize the independent features in a data in a fixed range or scale.
   There are multiple type of Feature Scaling, but we will Study two most usable : Normalization and Standardization
   applying feature scaling makes you compare two different type of column in a dataset

   Normalization
      It is the process of taking minimum inside the column, subtracting that minimum from every single value inside the column
      and then dividing by the difference between the maximum and minimum.
      And you will end up with the values ranging from 0 to 1. [0;1]

      X' = X - Xmin/(Xmax - Xmin)

   Standardization
      The process is similar instead of subtracting minimum we have to subtract the average and we divide by
      standard deviation.
      almost all the values lies between -3 to +3 or if we have outliers then they can end with the outside of -3to+3

      X' = X - u/sigma
         u -> average, sigma -> variance = underRoot(X - mean/(n-1))


Linear Regression
    Y = B0 + B1X1
    where -
       Y - Dependent variable
       B0 - y - Intercept
       B1 - Slope Coefficient
       X1 - Independent variable

    As we can see, there can be multiple slope lines
    that we can draw through our data points.
    And how do we know which one is the best one,
    which is the best linear regression,
    and, in fact, how do we even define the best one?
    So, in order to answer those questions,
    we need to look at a method
    called the ORDINARY LEAST SQUARE METHOD.
    And the way it works in a visual sense
    is we need to take our data points
    and project them vertically onto our linear regression line.
    Now, we would need to do this
    for every single linear regression line
    that we're considering,
    but for simplicity's sake in this tutorial,
    we're just going to do it with this line here in the middle.
    Now, for each pair of points, we have two values,
    yi and yi-hat.
    So, what are these values?
    yi is the actual amount of potatoes.
    In our case, in our example, potatoes yielded from the farm
    when that specific amount of nitrogen fertilizer was used.
    So, let's say 15 kilograms of nitrogen fertilizer were used
    and the farm yielded two tons of potatoes.
    yi-hat, on the other hand, is what this linear regression
    that we're considering,
    what it predicts the yield to be or to would have been.
    So, in this case,
    let's say again 15 kilograms of nitrogen were used,
    but the linear regression that we're looking at
    predicts that only one and a half tons of potatoes
    would have been yielded from the farm.
    As you can see, there's a slight difference
    between the actual value, actual yield,
    and the predicted yield.
    And that's normal.
    It's never going to be perfectly,
    this line's never gonna go perfectly through
    every single data point.
    That's simply impossible.
    But what we want to do is we want to find the best line,
    and it will be related to how small these differences are,
    as we can imagine.
    So, let's have a look here.
    There's our yi and yi-hat.
    The difference between them is called the residual.
    Here's our equation.
    And the best equation is such equation where b,
    or where such an equation where the parameters b0 and b1
    are such that the sum of the squares
    of the RESIDUALS is minimized.
    And that's why it's called
    the ORDINARY LEAST SQUARE METHOD.
    So, we need to take
    all of these residuals, these differences,
    we need to square them for every single data point,
    and then we need to add up the sum.
    And whenever we find the smallest value,
    so, for whichever regression line
    this value is going to be the smallest,
    that will be the best regression line.
    And that will guarantee that the line
    is going nicely through the data points,
    and it is the best line or the best linear regression
    to use for modeling our problem.
    So, there you go, that's an initial
    how the Ordinary Lease Squares method works.

Simple Linear Regression(Supervised Learning)
    Here regression means the output value will be numerical

    The big difference between regression
    and classification regression
    is when you have to predict
    a continuous real value, like a salary,
    as we're about to do.
    And classification is when you have to predict a category
    or you know, a class, which we will do
    in part three, Classification.
    
Simple Linear regression can be calculated in tewo ways:-
    closed from solution - this techinique is called Ordinary least square(this is simple)
        Linear Regression of python module contains OLS
        in this method problem increases when dimensions increases, bcz of which we use gradient diescent
        m(slope) = sum((Xi - X(bar))(Yi - Y(bar)))/(sum(Xi - X(bar)))^2
        
    non - closed form - this uses Gradient discent
       for this we have SGDRegressor in python
       

Assumptions Of Linear Regression->
   1. Linearity - Linear relationship between Y and each X
   2. Homoscedasticity -> Equal varience
   3. Multivariate Normality -> Normality of error Distribution
   4. Independence -> observation includes no autocorrelation(no pattern)
   5. Lack Of multicollinearity -> predictors are not correlated with each other.
   6. The Outlier Check -> This is not an assumption, but an extra
