{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfec96f",
   "metadata": {},
   "source": [
    "# R Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65296615",
   "metadata": {},
   "source": [
    "<img src=\"Image_Notes/T08_RSquared.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d472b79a",
   "metadata": {},
   "source": [
    "we're talking about the R squared, a very important concept when it comes to evaluating the goodness of fit of our models.\n",
    "Now, in order to understand R squared, we're going to need to look at two versions of our chart. So here is our data set,\n",
    "and what we're going to do here is plot the regression as we were doing before. And here's our data set, again, and this time we're going to just plot an average line. So we'll start with the regression. Let's draw our regression line.\n",
    "As usual, let's project vertically our data points onto it. And for each data point, we're going to look at the difference\n",
    "between yi, the actual value, and yi hat, the predicted value. Now as we discussed before, the way we built this line is\n",
    "we are minimizing this sum over here, and that is the ordinary least squares method. Well, actually this sum has a name,\n",
    "it's called the residual sum of squares, and it's indicated by this abbreviation. Now on the right here, we're going to draw an average line, and this is simply taking all of the y values, So the actual values of our data set, the y values,\n",
    "and taking their average. Again, we're going to project vertically our data points onto this line. And for each data point, we're going to look at yi. And here we're going to calculate another total. And this one is called the total sum of squares, and is indicated by this abbreviation. And it's similar to the residual sum squares, but instead of using yi hat, we're looking at the difference between yi, the actual value, and y average, the average value of our data set. And now we can calculate R squared. R squared is defined as one minus the ratio between the residual sum squares and the total sum of squares. the residual sum squares is usually, in most cases, it's less than the total sum of squares. So thereby, unless our regression model is facing absolutely the wrong way, for example, a downward slope on the left over here, if our model was sloping downwards, then the residual sum of squares would be huge because our model's just incorrect.\n",
    "But in all other cases the residual sum of squares is less than the total sum squares. And what that means is that the ratio is less than one. So R squared is somewhere between zero and one.\n",
    "And the better our model fits the data, the smaller the residual sum of squares will be, and that means the greater R squared will be. So here's a quick rule of thumb for R squared. Now bear in mind that it highly depends on the context,\n",
    "and this rule of thumb is just for the practical tutorials that we're looking at in this section of the course.\n",
    "And here we go. So if you have a one, R squared of one, that's a perfect fit. That means there residual sum squares is zero, and basically your line is going through all the data points, which is virtually impossible. So it's very suspicious.\n",
    "If your R squared is about 0.9, that's a very good model. If your R squared is less than 0.7, it's not a great model.\n",
    "It's not the end of the world, but not great. If it's less than 0.4, that's quite a terrible model. And if it's less than zero, then the model makes no sense for this data as we discussed. So there we go, that's how the R squared works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2628e8e",
   "metadata": {},
   "source": [
    "# Adjusted R Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e492f",
   "metadata": {},
   "source": [
    "<img src=\"Image_Notes/T08_AdjustedRSquare.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae2662",
   "metadata": {},
   "source": [
    "<img src=\"Image_Notes/T08_AdjustedRSquare2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3fd9a",
   "metadata": {},
   "source": [
    "However, there is an overarching problem across the board which is to do with adding new independent variables.\n",
    "So, let's say we have a linear regression with two independent variables and we decide to add a third one. For example, we got new data, a new column of data, or we got, we're just trying to explore and see what other variables might be helpful\n",
    "in the explanation in our model. So, what happens when we add another variable is that the total sum of squares doesn't change because it only depends on the average of the y actual values and doesn't depend on the y-hat values. But the residual sum of squares will change and in fact it will only either decrease or stay the same. The problem we are facing\n",
    "is that the residual sum of squares will never increase when we try to add another variable, and this might not be intuitive at first, so let's talk about it a little bit. The main reason for this is that we are using the Ordinary Least Squares method to build our models. And what the Ordinary Least Squares method does is it aims to minimize the residual sum of squares. So, let's try, see this in action. When we add this new variable, X3, the Ordinary Least Squared method is going to look for coefficient b3 that improve the yi-hat predicted values. As long as it finds a coefficient b3 where the yi-hat values are better than they were before, closer to the actual values, then the residual sum of squares will improve.\n",
    "It could improve by a lot if the prediction is much better now or it can improve by a tiny little bit, even if the prediction is a little bit better. Now in the situation where the Ordinary Least Squares method cannot find a coefficient b3 that improves the predictions, like all possible coefficients b3 make the predictions worse, then the Ordinary Least Squared method is just going to be very smart or sneaky you can call it. And it's just going to turn b3 into zero. It's just going to say, okay, we are going to set b3 at zero and that means even though we technically added an extra variable,\n",
    "it's not participating at all in the predictions because its coefficient is zero. So, in that case, the residual sum of squares won't change, will be exactly as it was before. So, we end up with a situation where we can just keep adding\n",
    "more and more variables that maybe have even nothing to do with our problem at hand. But by virtue of some random correlations, our R Squared in some cases will be improving, improving, improving.\n",
    "It never gets worse. So, the residual sum of squares will decrease. That means R Squared will increase. And that's a problem because we don't want to end up with models that have lots of variables that have nothing to do with that model,\n",
    "that are not really adding a lot of value, but in general are just increasing R Squared. So, what is the solution?\n",
    "The solution is a new version of R Squared, an Adjusted R Squared, and that's exactly what it's called. It is calculated with this scary looking formula But as you'll see from the practical tutorials with Adlan, it's not scary at all, and you'll be able to recreate this, actually, manually. So, here there are a couple of new parameters. k is the number of independent variables that are in our model, and n is the sample size. And the important thing here is to look at k.\n",
    "So if we, if k increases over here, then the denominator decreases. That means the whole ratio increases. And because it's being subtracted, that means Adjusted R Square decreases. So, that's the important point here, that this new formula penalizes us for adding additional variables. So, basically it's only worth adding an extra variable if R Squared, this original R Squared, if it increases substantially enough that compensates for this penalty. So, when you're adding new variables becomes something that has to be justified. If it's not justified, then the new variable's not worth adding.\n",
    "And that's what Adjusted R Squared is about. It's about making sure that we only add variables when they bring substantial improvement to our model. So, there you go, that's Adjusted R Squared."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
